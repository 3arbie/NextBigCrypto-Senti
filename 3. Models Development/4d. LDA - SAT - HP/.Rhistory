trControl = train_control)
predictionsNB <- predict(NBayes,
newdata = test[,2:ncol(test)])
cmNB <- table(test$bin, predictionsNB)
confusionMatrix(predictionsNB,test$bin)
metrics(cmNB)
SVM <- train(bin ~.,
data = train,
method = "svmLinear",
trControl = train_control)
predictionsSVM <- predict(SVM,
newdata = test[,2:ncol(test)])
cmSVM <- table(test$bin, predictionsSVM)
confusionMatrix(predictionsSVM,test$bin)
metrics(cmSVM)
########################################
# C5.0 tree
C5.0 <- train(bin ~.,
data = train,
method = "C5.0",
trControl = train_control)
predictionsC50 <- predict(C5.0,
newdata = test[,2:ncol(test)])
cmC50 <- table(test$bin, predictionsC50)
confusionMatrix(predictionsC50,test$bin)
metrics(cmC50) # acc 57%
C5.0 <- train(bin ~.,
data = train,
method = "C5.0",
trControl = train_control)
predictionsC50 <- predict(C5.0,
newdata = test[,2:ncol(test)])
cmC50 <- table(test$bin, predictionsC50)
confusionMatrix(predictionsC50,test$bin)
metrics(cmC50)
C5.0 <- train(bin ~.,
data = train,
method = "C5.0",
trControl = train_control,
seed = 1234)
predictionsC50 <- predict(C5.0,
newdata = test[,2:ncol(test)])
cmC50 <- table(test$bin, predictionsC50)
confusionMatrix(predictionsC50,test$bin)
metrics(cmC50)
C5.0 <- train(bin ~.,
data = train,
method = "C5.0",
trControl = train_control,
seed = 1908)
predictionsC50 <- predict(C5.0,
newdata = test[,2:ncol(test)])
cmC50 <- table(test$bin, predictionsC50)
confusionMatrix(predictionsC50,test$bin)
metrics(cmC50)
C5.0 <- train(bin ~.,
data = train,
method = "C5.0",
trControl = train_control,
seed = 1234)
predictionsC50 <- predict(C5.0,
newdata = test[,2:ncol(test)])
cmC50 <- table(test$bin, predictionsC50)
confusionMatrix(predictionsC50,test$bin)
metrics(cmC50)
warnings()
?train
##############################
# k-fold validation (10)
train_control <- trainControl(## 10-fold CV
method = "cv",
number = 10,
seeds = 1234)
#################################
# Base-line model
logitrain <- train
logitest <- test
logitrain$bin <- ifelse(logitrain$bin == 'up',1,0)
logitest$bin <- ifelse(logitest$bin == 'up',1,0)
LogiModel <- glm(bin ~.,
data = logitrain,
family = binomial(link = "logit"))
summary(LogiModel)
# Prediction
prediction.Logi <- predict(LogiModel,
newdata= logitest[,2:ncol(logitest)],
type = "response")
prediction.Logi
# Convert to up/down
prediction.Logi <- ifelse(prediction.Logi < 0.5,'down','up')
cmLogi <- table(test$bin, prediction.Logi)
metrics(cmLogi) # 59%
confusionMatrix(as.factor(prediction.Logi),test$bin)
NBayes <- train(bin ~.,
data = train,
laplace = 1,
method = "nb",
trControl = train_control)
##############################
# k-fold validation (10)
train_control <- trainControl(## 10-fold CV
method = "cv",
number = 10)
########################################
# Random Forest
set.seeds(1234)
RF <- train(bin ~.,
data = train,
method = "rf",
trControl = train_control)
########################################
# Random Forest
set.seed(1234)
RF <- train(bin ~.,
data = train,
method = "rf",
trControl = train_control)
predictionsRF <- predict(RF,
newdata = test[,2:ncol(test)])
cmRF <- table(test$bin, predictionsRF)
confusionMatrix(predictionsRF,test$bin)
metrics(cmRF)
########################################
# Naive Bayes
set.seed(1234)
NBayes <- train(bin ~.,
data = train,
laplace = 1,
method = "nb",
trControl = train_control)
predictionsNB <- predict(NBayes,
newdata = test[,2:ncol(test)])
cmNB <- table(test$bin, predictionsNB)
confusionMatrix(predictionsNB,test$bin)
metrics(cmNB)
set.seed(1234)
NBayes <- train(bin ~.,
data = train,
laplace = 1,
method = "nb",
trControl = train_control)
predictionsNB <- predict(NBayes,
newdata = test[,2:ncol(test)])
cmNB <- table(test$bin, predictionsNB)
confusionMatrix(predictionsNB,test$bin)
metrics(cmNB)
set.seed(1234)
C5.0 <- train(bin ~.,
data = train,
method = "C5.0",
trControl = train_control)
predictionsC50 <- predict(C5.0,
newdata = test[,2:ncol(test)])
cmC50 <- table(test$bin, predictionsC50)
confusionMatrix(predictionsC50,test$bin)
metrics(cmC50)
set.seed(1234)
SVM <- train(bin ~.,
data = train,
method = "svmLinear",
trControl = train_control)
predictionsSVM <- predict(SVM,
newdata = test[,2:ncol(test)])
cmSVM <- table(test$bin, predictionsSVM)
confusionMatrix(predictionsSVM,test$bin)
metrics(cmSVM)
# save.image('~/GitHub/NextBigCrypto-Senti/Models/LDA_ETH_2405.RData')
load('~/GitHub/NextBigCrypto-Senti/Models/LDA_ETH_2405.RData')
options(stringsAsFactors = FALSE)
#Set up working directory
setwd("~/GitHub/NextBigCrypto-Senti/")
# install packages if not available
packages <- c("ldatuning", #tuning LDA topic numbers
"readr", #read data
"lubridate", #date time conversion
"dplyr", #date manipulation
"tm", # text mining package
"textmineR",
"tidytext",
"topicmodels","tidyr",
"ggplot2", # plotting package
"lda","LDAvis","servr"
)
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
corp <- VCorpus(VectorSource(text)) # VCorpus compatible with n-gram analysis
# Unigram
frequencies <- DocumentTermMatrix(corp)
# Remove these words that are not used very often. Keep terms that appear in 1% or more of the dataset
sparse <- removeSparseTerms(frequencies, 0.99)
# rowTotals <- apply(sparse,1,sum) # Find the sum of words in each Document
# dtm.new   <- dtm[rowTotals> 0, ] # remove all docs without words
ui <- unique(sparse$i)
sparse.new <- sparse[ui,]
gc() # garbage collector
#########################################################################
# #  Create document-term-matrix
# dtm <- CreateDtm(text,
#                  doc_names = c(1:length(text)),
#                  ngram_window = c(1, 1),
#                  lower = FALSE,
#                  remove_punctuation = FALSE,
#                  remove_numbers = FALSE)
#
# gc() # garbage collector
#
# rowTotals <- rowSums(dtm)        # Find the sum of words in each Document
# dtm.new   <- dtm[rowTotals> 0, ] # remove all docs without words
# # Faster way to remove 0s in dtm
# ui <- unique(dtm$i)
# dtm.new <- dtm[ui,]
#########################################################################
result <- FindTopicsNumber(
sparse.new,
topics = seq(from = 2, to = 20, by = 2),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 1234),
mc.cores = 1L,
verbose = TRUE
)
gc()
############################################################
# minimization for Arun and Cao Juan
# maximization for Griffiths and Deveaud
FindTopicsNumber_plot(result)
result
# save.image('~/GitHub/NextBigCrypto-Senti/Models/LDA_ETH_2405.RData')
# load('~/GitHub/NextBigCrypto-Senti/Models/LDA_ETH_2405.RData')
rm(sparse,corp,df,frequencies) # remove abundant objects
k = 14
# sparse dtm func
df_lda <- topicmodels::LDA(sparse.new,
k,
method = "Gibbs",
control = list(seed = 1234))
# Word-topic probabilities
df_topics <- tidy(df_lda, matrix = "beta")
df_topics
df_top_terms <- df_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# 10 terms that are most common within each topic
dev.new()
df_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
## Build classifier
df_gamma <- tidy(df_lda, matrix = "gamma")
save.image('~/GitHub/NextBigCrypto-Senti/Models/LDA_ETH_2505.RData')
load('~/GitHub/NextBigCrypto-Senti/Models/LDA_ETH_2405.RData')
save.image('~/GitHub/NextBigCrypto-Senti/Models/LDA_ETH_2505.RData')
write_csv(df,'~/GitHub/NextBigCrypto-Senti/0. Datasets/ETH_clean_2405.csv')
# clear the environment
rm(list= ls())
gc()
# load packages and set options
options(stringsAsFactors = FALSE)
#Set up working directory
setwd("~/GitHub/NextBigCrypto-Senti/")
# install packages if not available
packages <- c("readr", #read data
"lubridate", #date time conversion
"dplyr", #date manipulation
"tm", # text mining package
"textmineR","openxlsx",
"tidytext",
"ggplot2", # plotting package
"quanteda", #kwic function search phrases
"xtable", "DT", #viewing data type from quanteda
"stringi", #string manipulation
"wordcloud","tidyquant",
"caTools","caret", "rpart", "h2o","e1071","RWeka",
"randomForest"
)
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
lapply(packages, require, character.only = TRUE)
BTC.clean <- read_csv('~/GitHub/NextBigCrypto-Senti/0. Datasets/BTC_clean_2405.csv')
BTC.clean$status_id <- as.character(BTC.clean$status_id)
BTC.clean$user_id <- as.character(BTC.clean$user_id)
BTC.clean <- read_csv('~/GitHub/NextBigCrypto-Senti/0. Datasets/BTC_clean_2205.csv')
BTC.clean$status_id <- as.character(BTC.clean$status_id)
BTC.clean$user_id <- as.character(BTC.clean$user_id)
# save.image('~/GitHub/NextBigCrypto-Senti/Models/LDA_ETH_2505.RData')
load('~/GitHub/NextBigCrypto-Senti/Models/LDA_ETH_2505.RData')
rm(list=setdiff(ls(), c("df_lda"))) #remove everything except BTC.clean
save.image(df_lda,'~/GitHub/NextBigCrypto-Senti/Models/BTC_LDA.RData')
save.image('~/GitHub/NextBigCrypto-Senti/Models/BTC_LDA.RData')
save.image('~/GitHub/NextBigCrypto-Senti/Models/ETH_LDA.RData')
# save.image('~/GitHub/NextBigCrypto-Senti/Models/LDA_ETH_2505.RData')
load('~/GitHub/NextBigCrypto-Senti/Models/LDA_BTC_2405.RData')
rm(list=setdiff(ls(), c("df_lda"))) #remove everything except BTC.clean
save.image('~/GitHub/NextBigCrypto-Senti/Models/BTC_LDA.RData')
BTC.clean <- read_csv('~/GitHub/NextBigCrypto-Senti/0. Datasets/BTC_clean_2205.csv')
BTC.clean$status_id <- as.character(BTC.clean$status_id)
BTC.clean$user_id <- as.character(BTC.clean$user_id)
source('~/GitHub/NextBigCrypto-Senti/2. Preprocessing/1. Preprocessing_TW.R')
BTC.clean$status_id <- as.character(BTC.clean$status_id)
BTC.clean$user_id <- as.character(BTC.clean$user_id)
load('~/GitHub/NextBigCrypto-Senti/Models/BTC_LDA.RData')
## Build classifier
df_gamma <- tidy(df_lda, matrix = "gamma")
df_gamma
# Assign topic back to document
df_classifications <- df_gamma %>%
group_by(document) %>%
top_n(1, gamma) %>%
ungroup()
df_classifications$document <- as.numeric(df_classifications$document)
# sort topics
df_classifications <- df_classifications[with(df_classifications, order(document)), ]
BTC.clean$topic <- NA
View(df_classifications)
# just take the 1st topic result
df_classifications <- df_classifications[!duplicates(df_classifications$document),]
df_classifications$document
str(df_classifications)
df_classifications[!duplicates(df_classifications$document),]
View(df_classifications)
duplicates(df_classifications$document)
# just take the 1st topic result
df_classifications <- df_classifications[!duplicated(df_classifications$document),]
View(df_classifications)
View(BTC.clean)
BTC.clean.p1 <- BTC.clean %>% filter(created_at < '2018-04-15')
for (i in df_classifications$document){
BTC.clean.p1$topic[i] <- df_classifications$topic[i]
}
BTC.clean.p2 <- BTC.clean - BTC.clean.p1
BTC.clean.p2 <- anti_join(BTC.clean,BTC.clean.p1, by = 'status_id')
# Preprocessing
corp <- VCorpus(VectorSource(BTC.clean.p2$processed)) # VCorpus compatible with n-gram analysis
# Apply topic model on new data (after 15.04.2018)
BTC.clean.p2 <- anti_join(BTC.clean,BTC.clean.p1, by = 'status_id')
# Preprocessing
corp <- VCorpus(VectorSource(BTC.clean.p2$processed)) # VCorpus compatible with n-gram analysis
# Unigram
frequencies <- DocumentTermMatrix(corp)
# Remove these words that are not used very often. Keep terms that appear in 0.5% or more of the dataset
sparse <- removeSparseTerms(frequencies, 0.995)
# Generate sparse matrix
ResultSparse <- as.data.frame(as.matrix(sparse))
colnames(ResultSparse) <- make.names(colnames(ResultSparse))
df_lda2 <- topicmodels::posterior(df_lda,ResultSparse)
gc()
ui <- unique(sparse$i)
sparse.new <- sparse[ui,]
df_lda2 <- topicmodels::posterior(df_lda,sparse.new)
Reassign <- function(lda){
df_lda <- lda
## Build classifier
df_gamma <- tidy(df_lda, matrix = "gamma")
df_gamma
# Assign topic back to document
df_classifications <- df_gamma %>%
group_by(document) %>%
top_n(1, gamma) %>%
ungroup()
df_classifications$document <- as.numeric(df_classifications$document)
# sort topics
df_classifications <- df_classifications[with(df_classifications, order(document)), ]
# just take the 1st topic result
df_classifications <- df_classifications[!duplicated(df_classifications$document),]
return(df_classifications)
}
df_classifications <- Reassign(df_lda2)
#### Reassign topics back to main df
BTC.clean.p2$topic <- NA
for (i in df_classifications$document){
BTC.clean.p2$topic[i] <- df_classifications$topic[i]
}
## Build classifier
df_gamma <- tidy(df_lda2, matrix = "gamma")
df_lda2$terms
df_lda2$topics
View(df_lda2)
top_topic_per_doc <- apply(df_lda2$topics, 1, which.max)
top_topic_per_doc
length(top_topic_per_doc)
length(BTC.clean.p2)
nrow(BTC.clean.p2)
# no need to remove sparse
ui <- unique(frequencies$i)
sparse.new <- frequencies[ui,]
rm(df_lda2)
predict.LDA <- topicmodels::posterior(df_lda,sparse.new)
top_topic_per_doc <- apply(predict.LDA$topics, 1, which.max)
#### Reassign topics back to main df
BTC.clean.p2$topic <- NA
for (i in nrow(top_topic_per_doc)){
BTC.clean.p2$topic[i] <- top_topic_per_doc[i]
}
View(BTC.clean.p2)
top_topic_per_doc[1]
View(BTC.clean.p2)
BTC.clean.p2$topic[1:10]
top_topic_per_doc[1:10]
for (i in length(top_topic_per_doc)){
BTC.clean.p2$topic[i] <- top_topic_per_doc[i]
}
View(BTC.clean.p2)
View(BTC.clean.p1)
length(top_topic_per_doc)
for (i in 1:length(top_topic_per_doc)){
BTC.clean.p2$topic[i] <- top_topic_per_doc[i]
}
BTC.clean <- rbind(BTC.clean.p1,BTC.clean.p2)
gc()
BTC.clean <- rbind(BTC.clean.p1,BTC.clean.p2)
rm(list=setdiff(ls(), c("BTC.clean"))) #remove everything except BTC.clean
gc()
#######################################################################################
#######################################################################################
#######################################################################################
###################################
#     Load SA models (trained)    #
###################################
# Using best model so far (GBM-tuned 080518)
h2o.init()
gbm.model.opt <- h2o.loadModel('./Models/H2O/final_grid_model_1')
# Implement batch-running (in process)
i <- 1
j <- 50000
while (i <= j) {
# Preprocessing
corp <- VCorpus(VectorSource(BTC.clean$processed[i:j])) # VCorpus compatible with n-gram analysis
# Unigram
frequencies <- DocumentTermMatrix(corp,
control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
# Remove these words that are not used very often. Keep terms that appear in 0.5% or more of the dataset
sparse <- removeSparseTerms(frequencies, 0.995)
# Generate sparse matrix
ResultSparse <- as.data.frame(as.matrix(sparse))
colnames(ResultSparse) <- make.names(colnames(ResultSparse))
# Convert to H2O format
fullH2O <- as.h2o(ResultSparse)
# Prediction by batches
predictionsGBM.opt <- as.data.frame(h2o.predict(gbm.model.opt,fullH2O))
if (i==1){BTC.senti <- cbind(BTC.clean[i:j,], sentiment.trained = predictionsGBM.opt$predict)}
if (i!=1){BTC.senti <- rbind(BTC.senti,cbind(BTC.clean[i:j,],sentiment.trained = predictionsGBM.opt$predict))}
print(paste0('Complete from ',i,' to ',j,' observations!'))
# increase i and j
i <- i + 50000
ifelse((j + 50000) <= nrow(BTC.clean),j <- j + 50000, j <- nrow(BTC.clean))
gc()
}
h2o.shutdown()
BTC.senti <- BTC.senti %>%
mutate(date = as_datetime(created_at))%>%
select(date, status_id, user_id, screen_name, text, processed,
botprob, sentiment.trained,
topic) # add "topic" feature
View(BTC.senti)
# Summarize base on sentiment each day
# Trained models
# 20.05 trigger 6/12/24hr
time.slot <- 24 # insert trigger
BTC.senti.trained <- BTC.senti %>%
dplyr::select(date,sentiment.trained) %>%
group_by(time = floor_date(date, paste0(time.slot,' hour')),
sentiment.trained,
topic) %>%
summarize(count = n())
BTC.senti.trained <- BTC.senti %>%
dplyr::select(date,sentiment.trained,topic) %>%
group_by(time = floor_date(date, paste0(time.slot,' hour')),
sentiment.trained,
topic) %>%
summarize(count = n())
View(BTC.senti.trained)
BTC.senti.trained <- BTC.senti %>%
dplyr::select(date,sentiment.trained,topic) %>%
group_by(time = floor_date(date, paste0(time.slot,' hour')),
sentiment.trained,
topic) %>%
summarize(count = n(), count.tp = n())
View(BTC.senti.trained)
BTC.senti.trained <- BTC.senti %>%
dplyr::select(date,sentiment.trained,topic) %>%
group_by(time = floor_date(date, paste0(time.slot,' hour')),
sentiment.trained,
topic) %>%
summarize(count = n())
# Get total + percentage of each class / day
BTC.senti.trained <- BTC.senti.trained %>%
group_by(time) %>%
mutate(countT = sum(count),
countT.tp=sum(count)) %>%
group_by(sentiment.trained) %>%
mutate(per = round(100* count/countT,2),
per.topic = round(100 *count.tp/countT.tp,2))
# Get total + percentage of each class / day
BTC.senti.trained <- BTC.senti.trained %>%
group_by(time) %>%
mutate(countT = sum(count),
countT.tp=sum(count)) %>%
group_by(sentiment.trained) %>%
mutate(per = round(100* count/countT,2),
per.topic = round(100 *count/countT.tp,2))
View(BTC.senti.trained)
setwd("~/GitHub/NextBigCrypto-Senti/3. Models Development/4. SAT-LDA - Sentiment (Trained)")
# Until HERE
save.image('~/GitHub/NextBigCrypto-Senti/3. Models Development/4. SAT-LDA - Sentiment (Trained)/BTC_SAT-LDA_2505.RData')
